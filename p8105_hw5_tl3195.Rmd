---
title: "p8105_hw5_tl3195"
author: "Tingyi Li"
date: "2023-11-14"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

#Problem 1

```{r}
homicide_df = 
  read_csv("./homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) |>
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries and variables including the victim sex, race, age, and name; the date the homicide was reported; and the location of the homicide. A `city_state` variable was created that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. Tulsa, AL, was excluded from the dataset since it is not a major US city which could be viewed as a kind of error. 

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
summarise_homicide_df = 
  homicide_df |>
  select(city_state, disposition, resolution) |>
  group_by(city_state) |>
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```


`prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in Baltimore, MD.

```{r}
Baltimore = 
  prop.test(
    x = filter(summarise_homicide_df, city_state == "Baltimore, MD") |> pull(hom_unsolved),
    n = filter(summarise_homicide_df, city_state == "Baltimore, MD") |> pull(hom_total)) 

broom::tidy(Baltimore) |>
  knitr::kable(digits = 3)
```

```{r}
test_results = 
  summarise_homicide_df |>
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>
  select(-prop_tests) |>
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Make a plot

```{r}
test_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```




#Problem 2

Import data and iterate over file names and read in data
```{r,show_col_types = FALSE}
hw5_data=
  tibble(list.files("./data"))|>
  mutate(files=paste(list.files("./data")))

read_data=function(x) {
  data=read_csv(paste0("./data/", x))|>
    mutate(files_names=x)
}

arm = map_df(hw5_data$files, read_data)

arm
```

Tidy the result
```{r}
arm_data=
  arm|>
  janitor::clean_names()|>
  gather(value = arm_value, key = week, week_1:week_8)|>
  mutate(
    week=as.integer(str_remove(week, "week_")),
    subject_id=as.integer(str_extract(files_names, "[0-9][0-9]")),
    file_names = ifelse(str_detect(files_names, "con"), "Control", "Experiment"),
  )|>
  mutate(across(.cols = c(files_names, week, subject_id), as.factor)) |>
  select(file_names, subject_id, week, arm_value)

arm_data
```


Make a spaghetti plot showing observations on each subject over time.
```{r}
arm_data|>
  ggplot(aes(week, arm_value, color=subject_id)) + 
  geom_point() + geom_line(aes(group=subject_id), alpha=0.5) +
  facet_grid(~file_names) +
  labs(x="week", 
       y="arm value",
       title="Arm values on each subject from week 1 to week 8 in two groups", col="subject id")

```

There appears an general increasing trend in arm values over 8 weeks in experiment group. In contrast, there is no clear pattern of arm values for each subject over 8 weeks in control group. Moreover, the largest arm value in control group is below 5.



#Problem 3

Generate 5000 datasets from the model
```{r}
set.seed(1)
n = 30
sigma = 5
mu_assigned = 0:6
alpha = 0.05
simulations = 5000

sim_results = map_dfr(mu_assigned, function(mu) {
  tibble(mu = mu,
         sim = map(1:simulations, ~t.test(rnorm(n, mu, sigma))),
         estimate = map_dbl(sim, ~broom::tidy(.x)$estimate),
         p_value = map_dbl(sim, ~broom::tidy(.x)$p.value),
         reject = p_value < alpha)
})
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis.
```{r}
proportion_results <- sim_results |>
  group_by(mu)|>
  summarise(avg_mu_hat = mean(estimate),
            avg_mu_hat_reject = mean(estimate[reject]),
            power = mean(reject))
plot = proportion_results|>
  ggplot(aes(x = mu, y = power)) + geom_point() + geom_line() +
  labs(x = "True mean",
       y = "Power")

plot
```

There is a positive association between effect size and power. Specfically, when the true mean increases from 0 to 6, the power of the test also increases. The power of the test seems to increase drastically when the true mean increases from 0 to 2 and starts to slow down from 2 to 4. The plot seems to have minimal additional impact on power when the true mean values between 4 to 6. This relationship shows a concept in hypothesis test that a larger effect size makes the probability of correctly rejecting the null hypothesis/detecting the true effect higher. 


Make a plot showing the average estimate of mu on the y axis and the true value of mu on the x axis. Overlay on the first the average estimate of mu only in samples for which the null was rejected on the y axis and the true value of mu on the x axis.

```{r}
estimate_plot <- proportion_results|>
  ggplot(aes(x=mu)) + 
  geom_point(aes(y = avg_mu_hat, color = "average estimate of mu")) +
  geom_line(aes(y = avg_mu_hat, color = "average estimate of mu")) +
  geom_point(aes(y = avg_mu_hat_reject, color = "average estimate of mu for rejected")) +
  geom_line(aes(y = avg_mu_hat_reject, color = "average estimate of mu for rejected")) +
  labs(
    x = "true mean",
    y = "average estimated mean")

estimate_plot
```


The sample average of mu_hat across tests for which the null is rejected is not approximately equal to the true value of mu. Specifically, the estimated estimated mean is higher than the true mean when the null hypothesis is rejected. This might be attributed to overestimate of effect size because of random sampling variability. 